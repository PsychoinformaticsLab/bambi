{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How GPs are implemented in {brms}?\n",
    "\n",
    "* http://paul-buerkner.github.io/brms/reference/gp.html\n",
    "\n",
    "**Function signature**\n",
    "\n",
    "* **...:**\tOne or more predictors for the GP.\n",
    "* **by:** A numeric or factor variable of the same length as each predictor. In the numeric vector case, the elements multiply the values returned by the GP. In the factor variable case, a separate GP is fitted for each factor level.\n",
    "* **k:** Optional number of basis functions for computing approximate GPs. If NA (the default), exact GPs are computed.\n",
    "* **cov:** Name of the covariance kernel. By default, the exponentiated-quadratic kernel \"exp_quad\" is used.\n",
    "* **iso:** A flag to indicate whether an isotropic (`TRUE`; the default) or a non-isotropic GP should be used. In the former case, the same amount of smoothing is applied to all predictors. In the latter case, predictors may have different smoothing. Ignored if only a single predictors is supplied.\n",
    "* **gr:** Logical; Indicates if auto-grouping should be used (defaults to TRUE). If enabled, observations sharing the same predictor values will be represented by the same latent variable in the GP. This will improve sampling efficiency drastically if the number of unique predictor combinations is small relative to the number of observations.\n",
    "* **cmc:** Logical; Only relevant if `by` is a factor. If TRUE (the default), cell-mean coding is used for the by-factor, that is one GP per level is estimated. If FALSE, contrast GPs are estimated according to the contrasts set for the by-factor.\n",
    "* **scale:** Logical; If TRUE (the default), predictors are scaled so that the maximum Euclidean distance between two points is 1. This often improves sampling speed and convergence. Scaling also affects the estimated length-scale parameters in that they resemble those of scaled predictors (not of the original predictors) if scale is TRUE.\n",
    "* **c:** Numeric value only used in approximate GPs. Defines the multiplicative constant of the predictors' range over which predictions should be computed. A good default could be c = 5/4 but we are still working on providing better recommendations.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**\n",
    "\n",
    "* Why would we want to use the `by` argument with a numeric variable?\n",
    "    * Potential answer: It's like an interaction between a GP and a numerical variable. This also makes sense when `by` is categorical.\n",
    "* How easy is to switch from an isotropic to a non-isotropic GP in PyMC?\n",
    "    * If I remember correctly the process is quite manual\n",
    "* How to set priors for the parameters of the GP?\n",
    "    * Lengthscale\n",
    "    * Standard deviation\n",
    "    * ...?\n",
    "* How to allow for different mean and covariance functions?\n",
    "    * What is the mean function in {brms}? It doesn't seem to allow for different mean functions\n",
    "* `brms` currently supports only the exponentiated-quadratic kernel. Is there a fundamental reason to do so, that we may be missing?\n",
    "* Does `gr` make sense for non-approximated GPs?\n",
    "    * First, what does it refer to? I think it means it combines many GPs into one?\n",
    "    * Second, does it make sense for non-approximated GPs? I think the answer is yes.\n",
    "    * I suspect this is not the same as sharing the basis when using HSGP\n",
    "    * Check with Bill"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comments**\n",
    "\n",
    "* I like having a `cov` argument where we pass the name of the covariance kernel.\n",
    "    * How would one pass kernel specific parameters? (which parameters one may want to tweak?)\n",
    "    * The con is the formula will be longer and harder to read. Let's keep thinking about better alternatives that don't make the formula too long.\n",
    "* The `gp()` function is used for both exact and approximated GPs (via HSGP).\n",
    "    * I think it's better to both `gp()` and `hsgp()` in our case."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resources and Examples**\n",
    "\n",
    "* https://bookdown.org/ajkurz/Statistical_Rethinking_recoded/adventures-in-covariance.html\n",
    "* https://github.com/paul-buerkner/brms/issues/412\n",
    "* https://discourse.mc-stan.org/t/auto-grouping-option-in-brms-gp/11154\n",
    "* https://projecteuclid.org/journals/bayesian-analysis/volume-5/issue-1/Bayesian-functional-ANOVA-modeling-using-Gaussian-process-prior-distributions/10.1214/10-BA505.full\n",
    "* https://discourse.mc-stan.org/t/auto-grouping-of-latent-variable-in-gaussian-process/14866/5\n",
    "* https://discourse.mc-stan.org/t/incorporating-distance-matrices-into-gaussian-process-in-brms/17978/12\n",
    "* https://discourse.mc-stan.org/t/recovering-lscale-in-a-two-dimensional-gaussian-process-regression/8458\n",
    "    * Experiment to recover parameter values\n",
    "* https://github.com/mike-lawrence/hgpr\n",
    "    * Hierarchical GP in Stan\n",
    "* https://discourse.mc-stan.org/t/sharing-a-to-me-neat-parameterization-for-gaussian-processes/21701/2\n",
    "    * Alternative parametrization to avoid problems with variance estimation\n",
    "* https://mc-stan.org/users/documentation/case-studies/icar_stan.html\n",
    "    * Similar than above, but a tutorial with spatial data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bambi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1008626715a3728f1d66c057190bde332dc65ea99517a942c5f170959679b77d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
